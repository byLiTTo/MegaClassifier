{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "from tensorflow.keras.metrics import *\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "import src.data.Dataset as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(data, window_size):\n",
    "    return np.convolve(data, np.ones(window_size) / window_size, mode=\"valid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "required_paths = [\"/ai4eutils\", \"/CameraTraps\", \"/yolov5\"]\n",
    "python_path = os.environ.get(\"PYTHONPATH\", \"\")\n",
    "root_path = os.getcwd()\n",
    "\n",
    "for path in required_paths:\n",
    "    if not any(p.endswith(path) for p in python_path.split(\":\")):\n",
    "        python_path += f\":{root_path}/data/external{path}\"\n",
    "\n",
    "os.environ[\"PYTHONPATH\"] = python_path\n",
    "\n",
    "!echo \"PYTHONPATH: $PYTHONPATH\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"MobileNetV2\"\n",
    "version = \"v.1.0\"\n",
    "complete_name = model_name + \"_\" + version\n",
    "\n",
    "model = load_model(f\"./models/{model_name}/{complete_name}.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_validation_df = pd.read_csv(\n",
    "    f\"./logs/{model_name}/{version}/train_validation_history.csv\"\n",
    ")\n",
    "\n",
    "train_metrics = train_validation_df[\n",
    "    [col for col in train_validation_df.columns if not col.startswith(\"val_\")]\n",
    "]\n",
    "\n",
    "validation_metrics = train_validation_df[\n",
    "    [col for col in train_validation_df.columns if col.startswith(\"val_\")]\n",
    "]\n",
    "\n",
    "test_metrics = pd.read_csv(f\"./logs/{model_name}/{version}/test_history.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 0\n",
    "train_data = (\n",
    "    moving_average(train_metrics[\"accuracy\"], window_size)\n",
    "    if window_size != 0\n",
    "    else train_metrics[\"accuracy\"]\n",
    ")\n",
    "validation_data = (\n",
    "    moving_average(validation_metrics[\"val_accuracy\"], window_size)\n",
    "    if window_size != 0\n",
    "    else validation_metrics[\"val_accuracy\"]\n",
    ")\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        y=train_data,\n",
    "        x=np.arange(len(train_data)),\n",
    "        mode=\"lines\",\n",
    "        name=\"Train\",\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        y=validation_data,\n",
    "        x=np.arange(len(validation_data)),\n",
    "        mode=\"lines\",\n",
    "        name=\"Validation\",\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=f\"Accuracy - {model_name} {version}\",\n",
    "    xaxis_title=\"Epochs\",\n",
    "    yaxis_title=\"Accuracy\",\n",
    "    legend_title=\"Metrics\",\n",
    "    template=\"seaborn\",\n",
    "    width=1200,\n",
    "    height=1000,\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 0\n",
    "train_data = (\n",
    "    moving_average(train_metrics[\"loss\"], window_size)\n",
    "    if window_size != 0\n",
    "    else train_metrics[\"loss\"]\n",
    ")\n",
    "validation_data = (\n",
    "    moving_average(validation_metrics[\"val_loss\"], window_size)\n",
    "    if window_size != 0\n",
    "    else validation_metrics[\"val_loss\"]\n",
    ")\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        y=train_data,\n",
    "        x=np.arange(len(train_data)),\n",
    "        mode=\"lines\",\n",
    "        name=\"Train\",\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        y=validation_data,\n",
    "        x=np.arange(len(validation_data)),\n",
    "        mode=\"lines\",\n",
    "        name=\"Validation\",\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=f\"Loss - {model_name} {version}\",\n",
    "    xaxis_title=\"Epochs\",\n",
    "    yaxis_title=\"Loss\",\n",
    "    legend_title=\"Metrics\",\n",
    "    template=\"seaborn\",\n",
    "    width=1200,\n",
    "    height=1000,\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 0\n",
    "train_data = (\n",
    "    moving_average(train_metrics[\"presision\"], window_size)\n",
    "    if window_size != 0\n",
    "    else train_metrics[\"presision\"]\n",
    ")\n",
    "validation_data = (\n",
    "    moving_average(validation_metrics[\"val_presision\"], window_size)\n",
    "    if window_size != 0\n",
    "    else validation_metrics[\"val_presision\"]\n",
    ")\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        y=train_data,\n",
    "        x=np.arange(len(train_data)),\n",
    "        mode=\"lines\",\n",
    "        name=\"Train\",\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        y=validation_data,\n",
    "        x=np.arange(len(validation_data)),\n",
    "        mode=\"lines\",\n",
    "        name=\"Validation\",\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=f\"Presision - {model_name} {version}\",\n",
    "    xaxis_title=\"Epochs\",\n",
    "    yaxis_title=\"Presision\",\n",
    "    legend_title=\"Metrics\",\n",
    "    template=\"seaborn\",\n",
    "    width=1200,\n",
    "    height=1000,\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 0\n",
    "train_data = (\n",
    "    moving_average(train_metrics[\"recall\"], window_size)\n",
    "    if window_size != 0\n",
    "    else train_metrics[\"recall\"]\n",
    ")\n",
    "validation_data = (\n",
    "    moving_average(validation_metrics[\"val_recall\"], window_size)\n",
    "    if window_size != 0\n",
    "    else validation_metrics[\"val_recall\"]\n",
    ")\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        y=train_data,\n",
    "        x=np.arange(len(train_data)),\n",
    "        mode=\"lines\",\n",
    "        name=\"Train\",\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        y=validation_data,\n",
    "        x=np.arange(len(validation_data)),\n",
    "        mode=\"lines\",\n",
    "        name=\"Validation\",\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=f\"Recall - {model_name} {version}\",\n",
    "    xaxis_title=\"Epochs\",\n",
    "    yaxis_title=\"Recall\",\n",
    "    legend_title=\"Metrics\",\n",
    "    template=\"seaborn\",\n",
    "    width=1200,\n",
    "    height=1000,\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 0\n",
    "train_data = (\n",
    "    moving_average(train_metrics[\"auc\"], window_size)\n",
    "    if window_size != 0\n",
    "    else train_metrics[\"auc\"]\n",
    ")\n",
    "validation_data = (\n",
    "    moving_average(validation_metrics[\"val_auc\"], window_size)\n",
    "    if window_size != 0\n",
    "    else validation_metrics[\"val_auc\"]\n",
    ")\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        y=train_data,\n",
    "        x=np.arange(len(train_data)),\n",
    "        mode=\"lines\",\n",
    "        name=\"Train\",\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        y=validation_data,\n",
    "        x=np.arange(len(validation_data)),\n",
    "        mode=\"lines\",\n",
    "        name=\"Validation\",\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=f\"Auc - {model_name} {version}\",\n",
    "    xaxis_title=\"Epochs\",\n",
    "    yaxis_title=\"Auc\",\n",
    "    legend_title=\"Metrics\",\n",
    "    template=\"seaborn\",\n",
    "    width=1200,\n",
    "    height=1000,\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular F1-Score\n",
    "train_metrics[\"f1_score\"] = (\n",
    "    2\n",
    "    * (train_metrics[\"presision\"] * train_metrics[\"recall\"])\n",
    "    / (train_metrics[\"presision\"] + train_metrics[\"recall\"])\n",
    ")\n",
    "validation_metrics[\"val_f1_score\"] = (\n",
    "    2\n",
    "    * (validation_metrics[\"val_presision\"] * validation_metrics[\"val_recall\"])\n",
    "    / (validation_metrics[\"val_presision\"] + validation_metrics[\"val_recall\"])\n",
    ")\n",
    "\n",
    "window_size = 0\n",
    "train_data = (\n",
    "    moving_average(train_metrics[\"f1_score\"], window_size)\n",
    "    if window_size != 0\n",
    "    else train_metrics[\"f1_score\"]\n",
    ")\n",
    "validation_data = (\n",
    "    moving_average(validation_metrics[\"val_f1_score\"], window_size)\n",
    "    if window_size != 0\n",
    "    else validation_metrics[\"val_f1_score\"]\n",
    ")\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        y=train_data,\n",
    "        x=np.arange(len(train_data)),\n",
    "        mode=\"lines\",\n",
    "        name=\"Train\",\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        y=validation_data,\n",
    "        x=np.arange(len(validation_data)),\n",
    "        mode=\"lines\",\n",
    "        name=\"Validation\",\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=f\"F1 Score - {model_name} {version}\",\n",
    "    xaxis_title=\"Epochs\",\n",
    "    yaxis_title=\"F1 Score\",\n",
    "    legend_title=\"Metrics\",\n",
    "    template=\"seaborn\",\n",
    "    width=1200,\n",
    "    height=1000,\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import tensorflow\n",
    "\n",
    "IMAGES_DATASET_PATH = os.path.abspath(\"./data/raw/emptyNonEmptyDataset_ETL\")\n",
    "TEST_SAMPLES_CSV = os.path.abspath(\"./data/raw/emptyNonEmptyDataset_ETL/28570Test.csv\")\n",
    "test_dataframe = dt.load_from_csv(TEST_SAMPLES_CSV)\n",
    "test_dataframe[\"file_name\"] = test_dataframe[\"file_name\"].apply(\n",
    "    lambda x: os.path.join(IMAGES_DATASET_PATH, x)\n",
    ")\n",
    "test_dataframe[\"binary_label\"] = test_dataframe[\"binary_label\"].astype(str)\n",
    "IMG_SIZE = (224, 224)\n",
    "BATCH_SIZE = 32\n",
    "normal_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=tensorflow.keras.applications.mobilenet_v2.preprocess_input\n",
    ")\n",
    "test_images = normal_datagen.flow_from_dataframe(\n",
    "    dataframe=test_dataframe,\n",
    "    x_col=\"file_name\",\n",
    "    y_col=\"binary_label\",\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode=\"binary\",\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "# Calcular probabilidades para el conjunto de prueba\n",
    "y_pred_prob = model.predict(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from sklearn.metrics import auc, roc_curve\n",
    "\n",
    "y_true = test_images.labels\n",
    "\n",
    "# Calcular la curva ROC\n",
    "fpr, tpr, thresholds = roc_curve(y_true, y_pred_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Graficar la curva ROC\n",
    "fig = go.Figure()\n",
    "\n",
    "# Curva ROC\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=fpr, y=tpr, mode=\"lines\", name=f\"{model_name}_{version} (AUC = {roc_auc:.2f})\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Línea de referencia (Random Guess)\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=[0, 1],\n",
    "        y=[0, 1],\n",
    "        mode=\"lines\",\n",
    "        name=\"Random Classifier\",\n",
    "        line=dict(dash=\"dash\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "# Configurar diseño\n",
    "fig.update_layout(\n",
    "    # title=\"Receiver Operating Characteristic (ROC)\",\n",
    "    title=f\"ROC Curve - {model_name} {version}\",\n",
    "    xaxis_title=\"False Positive Rate\",\n",
    "    yaxis_title=\"True Positive Rate\",\n",
    "    legend_title=\"Curves\",\n",
    "    template=\"seaborn\",\n",
    "    width=1200,\n",
    "    height=1000,\n",
    ")\n",
    "\n",
    "# Mostrar la gráfica\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Obtener las clases predichas a partir de las probabilidades\n",
    "y_pred_class = (y_pred_prob > 0.5).astype(int)  # Para clasificación binaria\n",
    "\n",
    "# Calcular la matriz de confusión\n",
    "conf_matrix = confusion_matrix(y_true, y_pred_class)\n",
    "\n",
    "# Crear etiquetas con los números\n",
    "conf_matrix_text = [[str(value) for value in row] for row in conf_matrix]\n",
    "\n",
    "# Crear el heatmap con Plotly\n",
    "fig = go.Figure(\n",
    "    data=go.Heatmap(\n",
    "        z=conf_matrix,\n",
    "        x=[\"No Animal\", \"Animal\"],  # Predicted labels\n",
    "        y=[\"No Animal\", \"Animal\"],  # Actual labels\n",
    "        text=conf_matrix_text,  # Añadir los valores como texto\n",
    "        texttemplate=\"%{text}\",  # Mostrar los valores en las celdas\n",
    "        colorscale=\"Blues\",\n",
    "        showscale=True,\n",
    "    )\n",
    ")\n",
    "\n",
    "# Configurar diseño\n",
    "fig.update_layout(\n",
    "    title=f\"Confusion Matrix - {model_name} {version}\",\n",
    "    xaxis_title=\"Prediction\",\n",
    "    yaxis_title=\"Dataset\",\n",
    "    xaxis=dict(tickmode=\"array\", tickvals=[0, 1], ticktext=[\"No Animal\", \"Animal\"]),\n",
    "    yaxis=dict(tickmode=\"array\", tickvals=[0, 1], ticktext=[\"No Animal\", \"Animal\"]),\n",
    "    template=\"seaborn\",\n",
    "    width=1000,\n",
    "    height=1000,\n",
    ")\n",
    "\n",
    "# Mostrar la figura\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "\n",
    "# Calcular totales de etiquetas reales\n",
    "total_positive = np.sum(y_true)  # Total de muestras positivas\n",
    "total_negative = len(y_true) - total_positive  # Total de muestras negativas\n",
    "\n",
    "# Crear la matriz de confusión ideal\n",
    "conf_matrix_ideal = np.array(\n",
    "    [\n",
    "        [total_negative, 0],  # Verdaderos negativos, Falsos positivos\n",
    "        [0, total_positive],  # Falsos negativos, Verdaderos positivos\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Crear etiquetas con los números\n",
    "conf_matrix_ideal_text = [[str(value) for value in row] for row in conf_matrix_ideal]\n",
    "\n",
    "# Crear el heatmap con Plotly\n",
    "fig = go.Figure(\n",
    "    data=go.Heatmap(\n",
    "        z=conf_matrix_ideal,\n",
    "        x=[\"No Animal\", \"Animal\"],  # Predicted labels\n",
    "        y=[\"No Animal\", \"Animal\"],  # Actual labels\n",
    "        text=conf_matrix_ideal_text,  # Añadir los valores como texto\n",
    "        texttemplate=\"%{text}\",  # Mostrar los valores en las celdas\n",
    "        colorscale=\"Greens\",\n",
    "        showscale=True,\n",
    "    )\n",
    ")\n",
    "\n",
    "# Configurar diseño\n",
    "fig.update_layout(\n",
    "    title=\"Ideal Confusion Matrix\",\n",
    "    xaxis_title=\"Prediction\",\n",
    "    yaxis_title=\"Dataset\",\n",
    "    xaxis=dict(tickmode=\"array\", tickvals=[0, 1], ticktext=[\"No Animal\", \"Animal\"]),\n",
    "    yaxis=dict(tickmode=\"array\", tickvals=[0, 1], ticktext=[\"No Animal\", \"Animal\"]),\n",
    "    template=\"seaborn\",\n",
    "    width=1000,\n",
    "    height=1000,\n",
    ")\n",
    "\n",
    "# Mostrar la figura\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TFG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
